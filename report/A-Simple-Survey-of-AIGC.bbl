% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{b1}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez, {\L}.~Kaiser, and I.~Polosukhin, ``Attention is all you need,'' \emph{Advances in neural information processing systems}, vol.~30, 2017.

\bibitem{b2}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova, ``Bert: Pre-training of deep bidirectional transformers for language understanding,'' \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{b3}
A.~Radford, K.~Narasimhan, T.~Salimans, I.~Sutskever \emph{et~al.}, ``Improving language understanding by generative pre-training,'' 2018.

\bibitem{b4}
A.~Radford, J.~Wu, R.~Child, D.~Luan, D.~Amodei, I.~Sutskever \emph{et~al.}, ``Language models are unsupervised multitask learners,'' \emph{OpenAI blog}, vol.~1, no.~8, p.~9, 2019.

\bibitem{b5}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal, A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell \emph{et~al.}, ``Language models are few-shot learners,'' \emph{Advances in neural information processing systems}, vol.~33, pp. 1877--1901, 2020.

\bibitem{b6}
OpenAI, ``Gpt-4 technical report,'' \emph{arXiv}, 2023.

\bibitem{b7}
K.~Knill and S.~Young, ``Hidden markov models in speech and language processing,'' \emph{Corpus-based methods in language and speech processing}, pp. 27--68, 1997.

\bibitem{b8}
D.~A. Reynolds \emph{et~al.}, ``Gaussian mixture models.'' \emph{Encyclopedia of biometrics}, vol. 741, no. 659-663, 2009.

\bibitem{b9}
Y.~Bengio, R.~Ducharme, and P.~Vincent, ``A neural probabilistic language model,'' \emph{Advances in neural information processing systems}, vol.~13, 2000.

\bibitem{b10}
T.~Mikolov, M.~Karafi{\'a}t, L.~Burget, J.~Cernock{\`y}, and S.~Khudanpur, ``Recurrent neural network based language model.'' in \emph{Interspeech}, vol.~2, no.~3.\hskip 1em plus 0.5em minus 0.4em\relax Makuhari, 2010, pp. 1045--1048.

\bibitem{b11}
A.~Graves and A.~Graves, ``Long short-term memory,'' \emph{Supervised sequence labelling with recurrent neural networks}, pp. 37--45, 2012.

\bibitem{b12}
R.~Dey and F.~M. Salem, ``Gate-variants of gated recurrent unit (gru) neural networks,'' in \emph{2017 IEEE 60th international midwest symposium on circuits and systems (MWSCAS)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2017, pp. 1597--1600.

\bibitem{b13}
U.~Khandelwal, H.~He, P.~Qi, and D.~Jurafsky, ``Sharp nearby, fuzzy far away: How neural language models use context,'' \emph{arXiv preprint arXiv:1805.04623}, 2018.

\bibitem{b14}
A.~A. Efros and T.~K. Leung, ``Texture synthesis by non-parametric sampling,'' in \emph{Proceedings of the seventh IEEE international conference on computer vision}, vol.~2.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 1999, pp. 1033--1038.

\bibitem{b15}
P.~S. Heckbert, ``Survey of texture mapping,'' \emph{IEEE computer graphics and applications}, vol.~6, no.~11, pp. 56--67, 1986.

\bibitem{goodfellow2020generative}
I.~Goodfellow, J.~Pouget-Abadie, M.~Mirza, B.~Xu, D.~Warde-Farley, S.~Ozair, A.~Courville, and Y.~Bengio, ``Generative adversarial networks,'' \emph{Communications of the ACM}, vol.~63, no.~11, pp. 139--144, 2020.

\bibitem{kingma2013auto}
D.~P. Kingma and M.~Welling, ``Auto-encoding variational bayes,'' \emph{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem{song2019generative}
Y.~Song and S.~Ermon, ``Generative modeling by estimating gradients of the data distribution,'' \emph{Advances in neural information processing systems}, vol.~32, 2019.

\bibitem{dosovitskiy2020image}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai, T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly \emph{et~al.}, ``An image is worth 16x16 words: Transformers for image recognition at scale,'' \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{liu2021swin}
Z.~Liu, Y.~Lin, Y.~Cao, H.~Hu, Y.~Wei, Z.~Zhang, S.~Lin, and B.~Guo, ``Swin transformer: Hierarchical vision transformer using shifted windows,'' in \emph{Proceedings of the IEEE/CVF international conference on computer vision}, 2021, pp. 10\,012--10\,022.

\bibitem{radford2021learning}
A.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, G.~Goh, S.~Agarwal, G.~Sastry, A.~Askell, P.~Mishkin, J.~Clark \emph{et~al.}, ``Learning transferable visual models from natural language supervision,'' in \emph{International conference on machine learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2021, pp. 8748--8763.

\bibitem{bahdanau2014neural}
D.~Bahdanau, K.~Cho, and Y.~Bengio, ``Neural machine translation by jointly learning to align and translate,'' \emph{arXiv preprint arXiv:1409.0473}, 2014.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun, ``Deep residual learning for image recognition,'' in \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, 2016, pp. 770--778.

\bibitem{ba2016layer}
J.~L. Ba, J.~R. Kiros, and G.~E. Hinton, ``Layer normalization,'' \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem{britz2017massive}
D.~Britz, A.~Goldie, M.-T. Luong, and Q.~Le, ``Massive exploration of neural machine translation architectures,'' \emph{arXiv preprint arXiv:1703.03906}, 2017.

\bibitem{liu2018generating}
P.~J. Liu, M.~Saleh, E.~Pot, B.~Goodrich, R.~Sepassi, L.~Kaiser, and N.~Shazeer, ``Generating wikipedia by summarizing long sequences,'' \emph{arXiv preprint arXiv:1801.10198}, 2018.

\bibitem{wang2023robustness}
J.~Wang, X.~Hu, W.~Hou, H.~Chen, R.~Zheng, Y.~Wang, L.~Yang, H.~Huang, W.~Ye, X.~Geng \emph{et~al.}, ``On the robustness of chatgpt: An adversarial and out-of-distribution perspective,'' \emph{arXiv preprint arXiv:2302.12095}, 2023.

\bibitem{shen2023chatgpt}
X.~Shen, Z.~Chen, M.~Backes, and Y.~Zhang, ``In chatgpt we trust? measuring and characterizing the reliability of chatgpt,'' \emph{arXiv preprint arXiv:2304.08979}, 2023.

\bibitem{liu2023evaluating}
H.~Liu, R.~Ning, Z.~Teng, J.~Liu, Q.~Zhou, and Y.~Zhang, ``Evaluating the logical reasoning ability of chatgpt and gpt-4,'' \emph{arXiv preprint arXiv:2304.03439}, 2023.

\bibitem{chen2021badnl}
X.~Chen, A.~Salem, D.~Chen, M.~Backes, S.~Ma, Q.~Shen, Z.~Wu, and Y.~Zhang, ``Badnl: Backdoor attacks against nlp models with semantic-preserving improvements,'' in \emph{Annual Computer Security Applications Conference}, 2021, pp. 554--569.

\bibitem{struppek2022rickrolling}
L.~Struppek, D.~Hintersdorf, and K.~Kersting, ``Rickrolling the artist: Injecting invisible backdoors into text-guided image generation models,'' \emph{arXiv preprint arXiv:2211.02408}, 2022.

\bibitem{shen2021backdoor}
L.~Shen, S.~Ji, X.~Zhang, J.~Li, J.~Chen, J.~Shi, C.~Fang, J.~Yin, and T.~Wang, ``Backdoor pre-trained models can transfer to all,'' \emph{arXiv preprint arXiv:2111.00197}, 2021.

\bibitem{kurita2020weight}
K.~Kurita, P.~Michel, and G.~Neubig, ``Weight poisoning attacks on pre-trained models,'' \emph{arXiv preprint arXiv:2004.06660}, 2020.

\bibitem{li2021hidden}
S.~Li, H.~Liu, T.~Dong, B.~Z.~H. Zhao, M.~Xue, H.~Zhu, and J.~Lu, ``Hidden backdoors in human-centric language models,'' in \emph{Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security}, 2021, pp. 3123--3140.

\bibitem{nelson2008exploiting}
B.~Nelson, M.~Barreno, F.~J. Chi, A.~D. Joseph, B.~I. Rubinstein, U.~Saini, C.~Sutton, J.~D. Tygar, and K.~Xia, ``Exploiting machine learning to subvert your spam filter.'' \emph{LEET}, vol.~8, no. 1-9, pp. 16--17, 2008.

\bibitem{carlini2023poisoning}
N.~Carlini, M.~Jagielski, C.~A. Choquette-Choo, D.~Paleka, W.~Pearce, H.~Anderson, A.~Terzis, K.~Thomas, and F.~Tram{\`e}r, ``Poisoning web-scale training datasets is practical,'' \emph{arXiv preprint arXiv:2302.10149}, 2023.

\bibitem{perez2022ignore}
F.~Perez and I.~Ribeiro, ``Ignore previous prompt: Attack techniques for language models,'' \emph{arXiv preprint arXiv:2211.09527}, 2022.

\bibitem{kang2023exploiting}
D.~Kang, X.~Li, I.~Stoica, C.~Guestrin, M.~Zaharia, and T.~Hashimoto, ``Exploiting programmatic behavior of llms: Dual-use through standard security attacks,'' \emph{arXiv preprint arXiv:2302.05733}, 2023.

\bibitem{deshpande2023toxicity}
A.~Deshpande, V.~Murahari, T.~Rajpurohit, A.~Kalyan, and K.~Narasimhan, ``Toxicity in chatgpt: Analyzing persona-assigned language models,'' \emph{arXiv preprint arXiv:2304.05335}, 2023.

\bibitem{maus2023adversarial}
N.~Maus, P.~Chao, E.~Wong, and J.~Gardner, ``Adversarial prompting for black box foundation models,'' \emph{arXiv preprint arXiv:2302.04237}, 2023.

\bibitem{li2023multi}
H.~Li, D.~Guo, W.~Fan, M.~Xu, and Y.~Song, ``Multi-step jailbreaking privacy attacks on chatgpt,'' \emph{arXiv preprint arXiv:2304.05197}, 2023.

\bibitem{greshake2023more}
K.~Greshake, S.~Abdelnabi, S.~Mishra, C.~Endres, T.~Holz, and M.~Fritz, ``More than you've asked for: A comprehensive analysis of novel prompt injection threats to application-integrated large language models,'' \emph{arXiv preprint arXiv:2302.12173}, 2023.

\bibitem{seshia2016towards}
S.~A. Seshia, D.~Sadigh, and S.~S. Sastry, ``Towards verified artificial intelligence,'' \emph{arXiv preprint arXiv:1606.08514}, 2016.

\bibitem{huang2017safety}
X.~Huang, M.~Kwiatkowska, S.~Wang, and M.~Wu, ``Safety verification of deep neural networks,'' in \emph{Computer Aided Verification: 29th International Conference, CAV 2017, Heidelberg, Germany, July 24-28, 2017, Proceedings, Part I 30}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2017, pp. 3--29.

\bibitem{sinha2017certifying}
A.~Sinha, H.~Namkoong, R.~Volpi, and J.~Duchi, ``Certifying some distributional robustness with principled adversarial training,'' \emph{arXiv preprint arXiv:1710.10571}, 2017.

\bibitem{goodfellow2017challenge}
I.~Goodfellow and N.~Papernot, ``The challenge of verification and testing of machine learning,'' \emph{Cleverhans-blog}, 2017.

\bibitem{wicker2018feature}
M.~Wicker, X.~Huang, and M.~Kwiatkowska, ``Feature-guided black-box safety testing of deep neural networks,'' in \emph{Tools and Algorithms for the Construction and Analysis of Systems: 24th International Conference, TACAS 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings, Part I 24}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2018, pp. 408--426.

\bibitem{wu2020game}
M.~Wu, M.~Wicker, W.~Ruan, X.~Huang, and M.~Kwiatkowska, ``A game-based approximate verification of deep neural networks with provable guarantees,'' \emph{Theoretical Computer Science}, vol. 807, pp. 298--329, 2020.

\bibitem{xu2022quantifying}
P.~Xu, W.~Ruan, and X.~Huang, ``Quantifying safety risks of deep neural networks,'' \emph{Complex \& Intelligent Systems}, pp. 1--18, 2022.

\bibitem{ruan2019global}
W.~Ruan, M.~Wu, Y.~Sun, X.~Huang, D.~Kroening, and M.~Kwiatkowska, ``Global robustness evaluation of deep neural networks with provable guarantees for the hamming distance.''\hskip 1em plus 0.5em minus 0.4em\relax International Joint Conferences on Artificial Intelligence Organization, 2019.

\bibitem{la2020assessing}
E.~La~Malfa, M.~Wu, L.~Laurenti, B.~Wang, A.~Hartshorn, and M.~Kwiatkowska, ``Assessing robustness of text classification through maximal safe radius computation,'' \emph{arXiv preprint arXiv:2010.02004}, 2020.

\bibitem{cheng2020seq2sick}
M.~Cheng, J.~Yi, P.-Y. Chen, H.~Zhang, and C.-J. Hsieh, ``Seq2sick: Evaluating the robustness of sequence-to-sequence models with adversarial examples,'' in \emph{Proceedings of the AAAI conference on artificial intelligence}, vol.~34, no.~04, 2020, pp. 3601--3608.

\bibitem{weng2022large}
Y.~Weng, M.~Zhu, S.~He, K.~Liu, and J.~Zhao, ``Large language models are reasoners with self-verification,'' \emph{arXiv preprint arXiv:2212.09561}, 2022.

\bibitem{weng2023neural}
Y.~Weng, M.~Zhu, F.~Xia, B.~Li, S.~He, K.~Liu, and J.~Zhao, ``Neural comprehension: Language models with compiled neural networks,'' \emph{arXiv preprint arXiv:2304.01665}, 2023.

\bibitem{frantar2022optimal}
E.~Frantar and D.~Alistarh, ``Optimal brain compression: A framework for accurate post-training quantization and pruning,'' \emph{arXiv preprint arXiv:2208.11580}, 2022.

\bibitem{feng2023survey}
P.~Feng and Z.~Tang, ``A survey of visual neural networks: current trends, challenges and opportunities,'' \emph{Multimedia Systems}, vol.~29, no.~2, pp. 693--724, 2023.

\bibitem{nagel2020up}
M.~Nagel, R.~A. Amjad, M.~Van~Baalen, C.~Louizos, and T.~Blankevoort, ``Up or down? adaptive rounding for post-training quantization,'' in \emph{International Conference on Machine Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2020, pp. 7197--7206.

\bibitem{yao2022zeroquant}
Z.~Yao, R.~Yazdani~Aminabadi, M.~Zhang, X.~Wu, C.~Li, and Y.~He, ``Zeroquant: Efficient and affordable post-training quantization for large-scale transformers,'' \emph{Advances in Neural Information Processing Systems}, vol.~35, pp. 27\,168--27\,183, 2022.

\bibitem{park2022nuqmm}
G.~Park, B.~Park, S.~J. Kwon, B.~Kim, Y.~Lee, and D.~Lee, ``nuqmm: Quantized matmul for efficient inference of large-scale generative language models,'' \emph{arXiv preprint arXiv:2206.09557}, 2022.

\bibitem{dettmers2022gpt3}
T.~Dettmers, M.~Lewis, Y.~Belkada, and L.~Zettlemoyer, ``Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale,'' \emph{Advances in Neural Information Processing Systems}, vol.~35, pp. 30\,318--30\,332, 2022.

\bibitem{hu2021lora}
E.~J. Hu, Y.~Shen, P.~Wallis, Z.~Allen-Zhu, Y.~Li, S.~Wang, L.~Wang, and W.~Chen, ``Lora: Low-rank adaptation of large language models,'' \emph{arXiv preprint arXiv:2106.09685}, 2021.

\end{thebibliography}
