% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{b1}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez, {\L}.~Kaiser, and I.~Polosukhin, ``Attention is all you need,'' \emph{Advances in neural information processing systems}, vol.~30, 2017.

\bibitem{b2}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova, ``Bert: Pre-training of deep bidirectional transformers for language understanding,'' \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{b3}
A.~Radford, K.~Narasimhan, T.~Salimans, I.~Sutskever \emph{et~al.}, ``Improving language understanding by generative pre-training,'' 2018.

\bibitem{b4}
A.~Radford, J.~Wu, R.~Child, D.~Luan, D.~Amodei, I.~Sutskever \emph{et~al.}, ``Language models are unsupervised multitask learners,'' \emph{OpenAI blog}, vol.~1, no.~8, p.~9, 2019.

\bibitem{b5}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal, A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell \emph{et~al.}, ``Language models are few-shot learners,'' \emph{Advances in neural information processing systems}, vol.~33, pp. 1877--1901, 2020.

\bibitem{b6}
OpenAI, ``Gpt-4 technical report,'' \emph{arXiv}, 2023.

\bibitem{b7}
K.~Knill and S.~Young, ``Hidden markov models in speech and language processing,'' \emph{Corpus-based methods in language and speech processing}, pp. 27--68, 1997.

\bibitem{b8}
D.~A. Reynolds \emph{et~al.}, ``Gaussian mixture models.'' \emph{Encyclopedia of biometrics}, vol. 741, no. 659-663, 2009.

\bibitem{b9}
Y.~Bengio, R.~Ducharme, and P.~Vincent, ``A neural probabilistic language model,'' \emph{Advances in neural information processing systems}, vol.~13, 2000.

\bibitem{b10}
T.~Mikolov, M.~Karafi{\'a}t, L.~Burget, J.~Cernock{\`y}, and S.~Khudanpur, ``Recurrent neural network based language model.'' in \emph{Interspeech}, vol.~2, no.~3.\hskip 1em plus 0.5em minus 0.4em\relax Makuhari, 2010, pp. 1045--1048.

\bibitem{b11}
A.~Graves and A.~Graves, ``Long short-term memory,'' \emph{Supervised sequence labelling with recurrent neural networks}, pp. 37--45, 2012.

\bibitem{b12}
R.~Dey and F.~M. Salem, ``Gate-variants of gated recurrent unit (gru) neural networks,'' in \emph{2017 IEEE 60th international midwest symposium on circuits and systems (MWSCAS)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2017, pp. 1597--1600.

\bibitem{b13}
U.~Khandelwal, H.~He, P.~Qi, and D.~Jurafsky, ``Sharp nearby, fuzzy far away: How neural language models use context,'' \emph{arXiv preprint arXiv:1805.04623}, 2018.

\bibitem{b14}
A.~A. Efros and T.~K. Leung, ``Texture synthesis by non-parametric sampling,'' in \emph{Proceedings of the seventh IEEE international conference on computer vision}, vol.~2.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 1999, pp. 1033--1038.

\bibitem{b15}
P.~S. Heckbert, ``Survey of texture mapping,'' \emph{IEEE computer graphics and applications}, vol.~6, no.~11, pp. 56--67, 1986.

\bibitem{goodfellow2020generative}
I.~Goodfellow, J.~Pouget-Abadie, M.~Mirza, B.~Xu, D.~Warde-Farley, S.~Ozair, A.~Courville, and Y.~Bengio, ``Generative adversarial networks,'' \emph{Communications of the ACM}, vol.~63, no.~11, pp. 139--144, 2020.

\bibitem{kingma2013auto}
D.~P. Kingma and M.~Welling, ``Auto-encoding variational bayes,'' \emph{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem{song2019generative}
Y.~Song and S.~Ermon, ``Generative modeling by estimating gradients of the data distribution,'' \emph{Advances in neural information processing systems}, vol.~32, 2019.

\bibitem{dosovitskiy2020image}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai, T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly \emph{et~al.}, ``An image is worth 16x16 words: Transformers for image recognition at scale,'' \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{liu2021swin}
Z.~Liu, Y.~Lin, Y.~Cao, H.~Hu, Y.~Wei, Z.~Zhang, S.~Lin, and B.~Guo, ``Swin transformer: Hierarchical vision transformer using shifted windows,'' in \emph{Proceedings of the IEEE/CVF international conference on computer vision}, 2021, pp. 10\,012--10\,022.

\bibitem{radford2021learning}
A.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, G.~Goh, S.~Agarwal, G.~Sastry, A.~Askell, P.~Mishkin, J.~Clark \emph{et~al.}, ``Learning transferable visual models from natural language supervision,'' in \emph{International conference on machine learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2021, pp. 8748--8763.

\bibitem{bahdanau2014neural}
D.~Bahdanau, K.~Cho, and Y.~Bengio, ``Neural machine translation by jointly learning to align and translate,'' \emph{arXiv preprint arXiv:1409.0473}, 2014.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun, ``Deep residual learning for image recognition,'' in \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, 2016, pp. 770--778.

\bibitem{ba2016layer}
J.~L. Ba, J.~R. Kiros, and G.~E. Hinton, ``Layer normalization,'' \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem{britz2017massive}
D.~Britz, A.~Goldie, M.-T. Luong, and Q.~Le, ``Massive exploration of neural machine translation architectures,'' \emph{arXiv preprint arXiv:1703.03906}, 2017.

\bibitem{liu2018generating}
P.~J. Liu, M.~Saleh, E.~Pot, B.~Goodrich, R.~Sepassi, L.~Kaiser, and N.~Shazeer, ``Generating wikipedia by summarizing long sequences,'' \emph{arXiv preprint arXiv:1801.10198}, 2018.

\end{thebibliography}
