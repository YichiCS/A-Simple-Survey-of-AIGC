@article{b1,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{b2,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
@article{b3,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={OpenAI}
}
@article{b4,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
@article{b5,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
@article{b6,
  title={GPT-4 technical report},
  author={OpenAI},
  journal={arXiv},
  year={2023}
}
@article{b7,
  title={Hidden Markov models in speech and language processing},
  author={Knill, K and Young, S},
  journal={Corpus-based methods in language and speech processing},
  pages={27--68},
  year={1997},
  publisher={Springer}
}

@article{b8,
  title={Gaussian mixture models.},
  author={Reynolds, Douglas A and others},
  journal={Encyclopedia of biometrics},
  volume={741},
  number={659-663},
  year={2009},
  publisher={Berlin, Springer}
}

@article{b9,
  title={A neural probabilistic language model},
  author={Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal},
  journal={Advances in neural information processing systems},
  volume={13},
  year={2000}
}

@inproceedings{b10,
  title={Recurrent neural network based language model.},
  author={Mikolov, Tomas and Karafi{\'a}t, Martin and Burget, Lukas and Cernock{\`y}, Jan and Khudanpur, Sanjeev},
  booktitle={Interspeech},
  volume={2},
  number={3},
  pages={1045--1048},
  year={2010},
  organization={Makuhari}
}

@article{b11,
  title={Long short-term memory},
  author={Graves, Alex and Graves, Alex},
  journal={Supervised sequence labelling with recurrent neural networks},
  pages={37--45},
  year={2012},
  publisher={Springer}
}

@inproceedings{b12,
  title={Gate-variants of gated recurrent unit (GRU) neural networks},
  author={Dey, Rahul and Salem, Fathi M},
  booktitle={2017 IEEE 60th international midwest symposium on circuits and systems (MWSCAS)},
  pages={1597--1600},
  year={2017},
  organization={IEEE}
}

@article{b13,
  title={Sharp nearby, fuzzy far away: How neural language models use context},
  author={Khandelwal, Urvashi and He, He and Qi, Peng and Jurafsky, Dan},
  journal={arXiv preprint arXiv:1805.04623},
  year={2018}
}

@inproceedings{b14,
  title={Texture synthesis by non-parametric sampling},
  author={Efros, Alexei A and Leung, Thomas K},
  booktitle={Proceedings of the seventh IEEE international conference on computer vision},
  volume={2},
  pages={1033--1038},
  year={1999},
  organization={IEEE}
}

@article{b15,
  title={Survey of texture mapping},
  author={Heckbert, Paul S},
  journal={IEEE computer graphics and applications},
  volume={6},
  number={11},
  pages={56--67},
  year={1986},
  publisher={IEEE}
}

@article{goodfellow2020generative,
  title={Generative adversarial networks},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Communications of the ACM},
  volume={63},
  number={11},
  pages={139--144},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}

@article{song2019generative,
  title={Generative modeling by estimating gradients of the data distribution},
  author={Song, Yang and Ermon, Stefano},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={10012--10022},
  year={2021}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@article{liu2018generating,
  title={Generating wikipedia by summarizing long sequences},
  author={Liu, Peter J and Saleh, Mohammad and Pot, Etienne and Goodrich, Ben and Sepassi, Ryan and Kaiser, Lukasz and Shazeer, Noam},
  journal={arXiv preprint arXiv:1801.10198},
  year={2018}
}

@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{britz2017massive,
  title={Massive exploration of neural machine translation architectures},
  author={Britz, Denny and Goldie, Anna and Luong, Minh-Thang and Le, Quoc},
  journal={arXiv preprint arXiv:1703.03906},
  year={2017}
}
@article{wang2023robustness,
  title={On the robustness of chatgpt: An adversarial and out-of-distribution perspective},
  author={Wang, Jindong and Hu, Xixu and Hou, Wenxin and Chen, Hao and Zheng, Runkai and Wang, Yidong and Yang, Linyi and Huang, Haojun and Ye, Wei and Geng, Xiubo and others},
  journal={arXiv preprint arXiv:2302.12095},
  year={2023}
}
@article{shen2023chatgpt,
  title={In chatgpt we trust? measuring and characterizing the reliability of chatgpt},
  author={Shen, Xinyue and Chen, Zeyuan and Backes, Michael and Zhang, Yang},
  journal={arXiv preprint arXiv:2304.08979},
  year={2023}
}
@article{liu2023evaluating,
  title={Evaluating the logical reasoning ability of chatgpt and gpt-4},
  author={Liu, Hanmeng and Ning, Ruoxi and Teng, Zhiyang and Liu, Jian and Zhou, Qiji and Zhang, Yue},
  journal={arXiv preprint arXiv:2304.03439},
  year={2023}
}

@inproceedings{chen2021badnl,
  title={Badnl: Backdoor attacks against nlp models with semantic-preserving improvements},
  author={Chen, Xiaoyi and Salem, Ahmed and Chen, Dingfan and Backes, Michael and Ma, Shiqing and Shen, Qingni and Wu, Zhonghai and Zhang, Yang},
  booktitle={Annual Computer Security Applications Conference},
  pages={554--569},
  year={2021}
}
@article{struppek2022rickrolling,
  title={Rickrolling the Artist: Injecting Invisible Backdoors into Text-Guided Image Generation Models},
  author={Struppek, Lukas and Hintersdorf, Dominik and Kersting, Kristian},
  journal={arXiv preprint arXiv:2211.02408},
  year={2022}
}
@article{shen2021backdoor,
  title={Backdoor pre-trained models can transfer to all},
  author={Shen, Lujia and Ji, Shouling and Zhang, Xuhong and Li, Jinfeng and Chen, Jing and Shi, Jie and Fang, Chengfang and Yin, Jianwei and Wang, Ting},
  journal={arXiv preprint arXiv:2111.00197},
  year={2021}
}
@article{kurita2020weight,
  title={Weight poisoning attacks on pre-trained models},
  author={Kurita, Keita and Michel, Paul and Neubig, Graham},
  journal={arXiv preprint arXiv:2004.06660},
  year={2020}
}
@inproceedings{li2021hidden,
  title={Hidden backdoors in human-centric language models},
  author={Li, Shaofeng and Liu, Hui and Dong, Tian and Zhao, Benjamin Zi Hao and Xue, Minhui and Zhu, Haojin and Lu, Jialiang},
  booktitle={Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security},
  pages={3123--3140},
  year={2021}
}
@article{nelson2008exploiting,
  title={Exploiting machine learning to subvert your spam filter.},
  author={Nelson, Blaine and Barreno, Marco and Chi, Fuching Jack and Joseph, Anthony D and Rubinstein, Benjamin IP and Saini, Udam and Sutton, Charles and Tygar, J Doug and Xia, Kai},
  journal={LEET},
  volume={8},
  number={1-9},
  pages={16--17},
  year={2008}
}
@article{carlini2023poisoning,
  title={Poisoning web-scale training datasets is practical},
  author={Carlini, Nicholas and Jagielski, Matthew and Choquette-Choo, Christopher A and Paleka, Daniel and Pearce, Will and Anderson, Hyrum and Terzis, Andreas and Thomas, Kurt and Tram{\`e}r, Florian},
  journal={arXiv preprint arXiv:2302.10149},
  year={2023}
}
@article{perez2022ignore,
  title={Ignore Previous Prompt: Attack Techniques For Language Models},
  author={Perez, F{\'a}bio and Ribeiro, Ian},
  journal={arXiv preprint arXiv:2211.09527},
  year={2022}
}
@article{kang2023exploiting,
  title={Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks},
  author={Kang, Daniel and Li, Xuechen and Stoica, Ion and Guestrin, Carlos and Zaharia, Matei and Hashimoto, Tatsunori},
  journal={arXiv preprint arXiv:2302.05733},
  year={2023}
}
@article{deshpande2023toxicity,
  title={Toxicity in chatgpt: Analyzing persona-assigned language models},
  author={Deshpande, Ameet and Murahari, Vishvak and Rajpurohit, Tanmay and Kalyan, Ashwin and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2304.05335},
  year={2023}
}
@article{maus2023adversarial,
  title={Adversarial prompting for black box foundation models},
  author={Maus, Natalie and Chao, Patrick and Wong, Eric and Gardner, Jacob},
  journal={arXiv preprint arXiv:2302.04237},
  year={2023}
}
@article{li2023multi,
  title={Multi-step Jailbreaking Privacy Attacks on ChatGPT},
  author={Li, Haoran and Guo, Dadi and Fan, Wei and Xu, Mingshi and Song, Yangqiu},
  journal={arXiv preprint arXiv:2304.05197},
  year={2023}
}
@article{greshake2023more,
  title={More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models},
  author={Greshake, Kai and Abdelnabi, Sahar and Mishra, Shailesh and Endres, Christoph and Holz, Thorsten and Fritz, Mario},
  journal={arXiv preprint arXiv:2302.12173},
  year={2023}
}

@article{seshia2016towards,
  title={Towards verified artificial intelligence},
  author={Seshia, Sanjit A and Sadigh, Dorsa and Sastry, S Shankar},
  journal={arXiv preprint arXiv:1606.08514},
  year={2016}
}
@article{sinha2017certifying,
  title={Certifying some distributional robustness with principled adversarial training},
  author={Sinha, Aman and Namkoong, Hongseok and Volpi, Riccardo and Duchi, John},
  journal={arXiv preprint arXiv:1710.10571},
  year={2017}
}
@inproceedings{huang2017safety,
  title={Safety verification of deep neural networks},
  author={Huang, Xiaowei and Kwiatkowska, Marta and Wang, Sen and Wu, Min},
  booktitle={Computer Aided Verification: 29th International Conference, CAV 2017, Heidelberg, Germany, July 24-28, 2017, Proceedings, Part I 30},
  pages={3--29},
  year={2017},
  organization={Springer}
}
@article{goodfellow2017challenge,
  title={The challenge of verification and testing of machine learning},
  author={Goodfellow, Ian and Papernot, Nicolas},
  journal={Cleverhans-blog},
  year={2017}
}
@article{la2020assessing,
  title={Assessing robustness of text classification through maximal safe radius computation},
  author={La Malfa, Emanuele and Wu, Min and Laurenti, Luca and Wang, Benjie and Hartshorn, Anthony and Kwiatkowska, Marta},
  journal={arXiv preprint arXiv:2010.02004},
  year={2020}
}
@inproceedings{ruan2019global,
  title={Global robustness evaluation of deep neural networks with provable guarantees for the hamming distance},
  author={Ruan, Wenjie and Wu, Min and Sun, Youcheng and Huang, Xiaowei and Kroening, Daniel and Kwiatkowska, Marta},
  year={2019},
  organization={International Joint Conferences on Artificial Intelligence Organization}
}
@inproceedings{wicker2018feature,
  title={Feature-guided black-box safety testing of deep neural networks},
  author={Wicker, Matthew and Huang, Xiaowei and Kwiatkowska, Marta},
  booktitle={Tools and Algorithms for the Construction and Analysis of Systems: 24th International Conference, TACAS 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessaloniki, Greece, April 14-20, 2018, Proceedings, Part I 24},
  pages={408--426},
  year={2018},
  organization={Springer}
}
@article{wu2020game,
  title={A game-based approximate verification of deep neural networks with provable guarantees},
  author={Wu, Min and Wicker, Matthew and Ruan, Wenjie and Huang, Xiaowei and Kwiatkowska, Marta},
  journal={Theoretical Computer Science},
  volume={807},
  pages={298--329},
  year={2020},
  publisher={Elsevier}
}
@article{xu2022quantifying,
  title={Quantifying safety risks of deep neural networks},
  author={Xu, Peipei and Ruan, Wenjie and Huang, Xiaowei},
  journal={Complex \& Intelligent Systems},
  pages={1--18},
  year={2022},
  publisher={Springer}
}
@inproceedings{cheng2020seq2sick,
  title={Seq2sick: Evaluating the robustness of sequence-to-sequence models with adversarial examples},
  author={Cheng, Minhao and Yi, Jinfeng and Chen, Pin-Yu and Zhang, Huan and Hsieh, Cho-Jui},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={04},
  pages={3601--3608},
  year={2020}
}
@article{weng2022large,
  title={Large Language Models are reasoners with Self-Verification},
  author={Weng, Yixuan and Zhu, Minjun and He, Shizhu and Liu, Kang and Zhao, Jun},
  journal={arXiv preprint arXiv:2212.09561},
  year={2022}
}
@article{weng2023neural,
  title={Neural Comprehension: Language Models with Compiled Neural Networks},
  author={Weng, Yixuan and Zhu, Minjun and Xia, Fei and Li, Bin and He, Shizhu and Liu, Kang and Zhao, Jun},
  journal={arXiv preprint arXiv:2304.01665},
  year={2023}
}
@inproceedings{nagel2020up,
  title={Up or down? adaptive rounding for post-training quantization},
  author={Nagel, Markus and Amjad, Rana Ali and Van Baalen, Mart and Louizos, Christos and Blankevoort, Tijmen},
  booktitle={International Conference on Machine Learning},
  pages={7197--7206},
  year={2020},
  organization={PMLR}
}
@article{feng2023survey,
  title={A survey of visual neural networks: current trends, challenges and opportunities},
  author={Feng, Ping and Tang, Zhenjun},
  journal={Multimedia Systems},
  volume={29},
  number={2},
  pages={693--724},
  year={2023},
  publisher={Springer}
}
@article{frantar2022optimal,
  title={Optimal Brain Compression: A framework for accurate post-training quantization and pruning},
  author={Frantar, Elias and Alistarh, Dan},
  journal={arXiv preprint arXiv:2208.11580},
  year={2022}
}
@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}
@article{dettmers2022gpt3,
  title={Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30318--30332},
  year={2022}
}
@article{park2022nuqmm,
  title={nuqmm: Quantized matmul for efficient inference of large-scale generative language models},
  author={Park, Gunho and Park, Baeseong and Kwon, Se Jung and Kim, Byeongwook and Lee, Youngjoo and Lee, Dongsoo},
  journal={arXiv preprint arXiv:2206.09557},
  year={2022}
}
@article{yao2022zeroquant,
  title={ZeroQuant: Efficient and affordable post-training quantization for large-scale transformers},
  author={Yao, Zhewei and Yazdani Aminabadi, Reza and Zhang, Minjia and Wu, Xiaoxia and Li, Conglong and He, Yuxiong},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27168--27183},
  year={2022}
}