\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
%\usepackage{fontenc}
\usepackage[backref]{hyperref}
%\setmainfont{Times New Roman}
%\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
%    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{A Simple Survey of AIGC\\
%{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
%should not be used}
%\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Yichi Zhang }\\
\IEEEauthorblockA{\textit{Zhejiang University}\\
Hangzhou, China \\
yichics02@gmail.com}
}

\maketitle

\begin{abstract}
AIGC refers to Artificial Intelligence Generated Content, 
which is generated automatically or collaboratively using 
artificial intelligence techniques in response to user input 
or demand, including text, images, audio, video, and other forms 
of content. In this survey, the basic principles of Transformer 
and diffusion models are presented. The GPT and BERT models 
based on Transformer are introduced, as well as three different 
types of diffusion models and their practical applications. 
Finally, the security and robustness issues of AIGC models are 
discussed.
\end{abstract}

\begin{IEEEkeywords}
AIGC, Transformer, Diffusion, Security, Robustness
\end{IEEEkeywords}

\section{Introduction}
This survey will cover the following contains: First, 
we will focus on explaining the basic principles of the Transformer\cite{b1}, 
followed by a discussion of the details of language models (LLMs) 
that use the Transformer, such as BERT\cite{b2}, GPT\cite{b3, b4, b5, b6}, etc. 
We will also cover some techniques for training and optimizing 
these models. 
Finally, we will discuss the security and 
robustness of GPT models, respectively, 
based on recent research findings.

\section{Transformer}
\par Generative models have a long history in artificial intelligence, dating back to the 1950s with the
development of Hidden Markov Models (HMMs)\cite{b7} and Gaussian Mixture Models (GMMs)\cite{b8}.
These models generated sequential data such as speech and time series. However, it wasnâ€™t until
the advent of deep learning that generative models saw significant improvements in performance.
In early years of deep generative models, different areas do not have much overlap in general.
In natural language processing (NLP), a traditional method to generate sentences is to learn
word distribution using N-gram language modeling\cite{b9} and then search for the best sequence.
However, this method cannot effectively adapt to long sentences. To solve this problem, recurrent
neural networks (RNNs)\cite{b10} were later introduced for language modeling tasks, allowing for
modeling relatively long dependency. This was followed by the development of Long Short-Term
Memory (LSTM)\cite{b11} and Gated Recurrent Unit (GRU)\cite{b12}, which leveraged gating mechanism to
control memory during training. These methods are capable of attending to around 200 tokens in a
sample\cite{b13}, which marks a significant improvement compared to N-gram language models.

\par Meanwhile, in computer vision (CV), before the advent of deep learning-based methods, tra-
ditional image generation algorithms used techniques such as texture synthesis\cite{b14} and texture
mapping\cite{b15}. These algorithms were based on hand-designed features, and were limited in their abil-
ity to generate complex and diverse images. In 2014, Generative Adversarial Networks (GANs)\cite{goodfellow2020generative}
was first proposed, which was a significant milestone in this area, due to its impressive results
in various applications. Variational Autoencoders (VAEs)\cite{kingma2013auto} and other methods like diffusion
generative models\cite{song2019generative} have also been developed for more fine-grained control over the image
generation process and the ability to generate high-quality images.
\par The advancement of generative models in various domains has followed different paths, but
eventually, the intersection emerged: the transformer architecture\cite{b1}. The Transformer was 
initially developed to handle NLP tasks, but it has also been applied to tasks in computer vision. 
Its various variants for different tasks quickly achieved state-of-the-art performance. In the field of NLP, many
prominent large language models, e.g., BERT and GPT, adopt the transformer architecture as
their primary building block. In CV, Vision Transformer (ViT)\cite{dosovitskiy2020image} and Swin Transformer\cite{liu2021swin} 
later takes this concept even further by combining the transformer architecture with visual components, 
allowing it to be applied to image based downstreams. 
Except for the improvement that transformer brought
to individual modalities, this intersection also enabled models from different domains to be fused
together for multimodal tasks. One such example of multimodal models is CLIP\cite{radford2021learning}. CLIP is a
joint vision-language model that combines the transformer architecture with visual components,
allowing it to be trained on a massive amount of text and image data. Since it combines visual
and language knowledge during pre-training, it can also be used as image encoders in multimodal
prompting for generation. In all, the emergence of transformer based models revolutionized AI
generation and led to the possibility of large-scale training.

\subsection{Model Architecture of Transformer}
\par Here is a diagram of the Transformer model architecture. 
We will now explain this architecture by focusing on several modules: 
Encoder-Decoder\cite{bahdanau2014neural}, Self-Attention, Position-wise Feed-Forward Network, 
Embeddings, and Softmax.

\begin{figure}[htbp]
    \centerline{\includegraphics[width = 8cm]{pic/fig1.png}}
    \caption{Model Architecture of Transformer}
    \label{fig}
\end{figure}

\subsubsection{Encoder-Decoder}

\par The Encoder consists of $N$ layers, each of which contains two sub-layers. 
The first sub-layer includes a Multi-Head Attention module, and the 
second sub-layer includes a position-wise fully connected feed-forward network. 
In each sub-layer, we apply a residual connection\cite{he2016deep} module, followed by layer normalization\cite{ba2016layer}.

\par The Decoder also consists of $N$ independent layers, but unlike the Encoder, 
each layer of the Decoder contains three sub-layers, which has three different kinds of modules: 
Masked Multi-Head Attention, Multi-Head Attention, and position-wise fully connected 
feed-forward network. Similarly, we apply the same residual connection and layer 
normalization after each sub-layer.

\par This results in the following equation for each sub-layer:
\begin{align*}
    Output(x) = LayerNorm(x + SubLayer(x))
\end{align*}

where $Sublayer(x)$ is the function implemented by the sub-layer itself.


\subsubsection{Attention}
\par An attention function can be described as mapping a query and a key-value pair 
to a set of outputs, where all of these quantities are vectors. This allows the output 
to be viewed as a weighted sum of values, where each value's weight can be seen as 
calculated based on the query and its corresponding key.

\begin{figure}[htbp]
    \centerline{\includegraphics[width = 8cm]{pic/fig2.png}}
    \caption{Attention Mechanism}
    \label{fig}
\end{figure}

\par From Fig2, we can see that the Multi-Head Attention used in the model consists of 
multiple Scaled Dot-Product Attention modules. Therefore, let's focus on 
Scaled Dot-Product Attention first.
\begin{align*}
    Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
\end{align*}

\par Compared to regular dot-product attention, Scaled Dot-Product Attention includes a 
scaling factor $\sqrt{d_k}$\cite{britz2017massive} in the equation to improve the performance of the softmax 
function on larger datasets.

\par Next, let's consider how to apply Scaled Dot-Product Attention to Multi-Head Attention. 
For a set of $K$, $Q$, $V$, each with a $d_{model}$ dimension, they are projected to different 
dimensions ($d_k$, $d_k$, and $d_v$, respectively). Then, attention calculations are 
performed in parallel on the projected versions of the $Q$, $K$, and $V$, 
resulting in an output vector with a dimension of $d_v$.
\begin{align*}
    MultiHead&(Q, K, V) = Concat(head_1, head_2, ... , head_h)W^O \\
    where \  &head_i = Attention(QW^Q_i, KW^K_i, VW^V_i)\\
    &W^Q_i \in \mathbb{R}^{d_{model}\times d_k}, \  
    W^K_i \in \mathbb{R}^{d_{model}\times d_k}\\
    &W^V_i \in \mathbb{R}^{d_{model}\times d_v}, \ 
    W^O \in \mathbb{R}^{hd_v \times d_{model}}
\end{align*}

\par Where $h$ is the number of attention layers that are computing in parallel.

\par Note that in Fig2, there is also an optional mask module used to mask out certain 
invalid positions or information when calculating attention scores. This is done 
to prevent these invalid positions or information from being considered when calculating 
attention weights.

\subsubsection{Position-wise Feed-Forward Networks}

\par In each layer of the Encoder or the Decoder, there is a fully connected feed-forward 
network that is applied  separately and identically to each position. This network consists of 
two layers of ReLU activation functions combined together.
\begin{align*}
    FFN(x) = max(0, xW_1 + b_1)W_2 + b_2
\end{align*}

\par Of course, different layers may use different parameters, and there may be changes 
in dimensions caused by modules such as convolutional layers between layers.

\subsubsection{Embeddings and Softmax}

\par Like other sequence processing models, the Transformer also uses a learnable 
embedding to convert input and output tokens into $d_{model}$-dimensional vectors. It also 
uses a learnable linear transformer and softmax function to convert the decoder's output 
into predicted probabilities for the next token.In the embedding layers, 
multiply those weights by $\sqrt{d_{model}}$.

\subsubsection{Positional Encoding}

\par Since the Transformer does not contain convolutional or recurrent components, 
position encoding is required to incorporate positional information of the input sequence. 
Here, the position encoding uses the same dimension as the embedding to enable addition of the two. 
Given that there are various forms of position encoding, both learnable and fixed, 
the Transformer uses sine and cosine functions of different frequencies to encode position information.

\begin{align*}
    PE_{(pos,2i)} &= \sin(pos/10000^{2i/d_{model}})\\
    PE_{(pos,2i+1)} &= \cos(pos/10000^{2i/d_{model}})
\end{align*}

\par where $pos$ is the position and $i$ is the dimension.


\subsection{Training of Transformer}
\par In the original Transformer paper, the model was trained on the standard WMT 2014 English-German 
dataset using the Adam optimizer with hyperparameters $\beta_1 = 0.9$, $\beta_2 = 0.98$, and 
$ \epsilon = 10^{-9}$. The learning rate was assigned based on a formula, where the number 
of $warmup\_steps$ was set to 4000.

\begin{align*}
    lrate = \frac{min({step\_num}^{0.5}, step\_num \cdot warmup\_steps^{-1.5})}{\sqrt{d_{model}}}
\end{align*}

\par In addition, to prevent overfitting during training, several regularization 
techniques were applied, such as Residual Dropout with $P_{drop} = 0.1$ and label 
smoothing with a value of $\epsilon_{ps} = 0.1$. The use of these techniques allowed 
the Transformer to achieve better performance after training.

\section{BERT}

\par BERT\cite{b2}, which stands for Bidirectional Encoder Representations fromTransformers. 
Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), 
BERT is designed to pre-train deep bidirectional representations from
unlabeled text by jointly conditioning on both left and right context in all layers. As a 
result, the pre-trained BERT model can be fine-tuned with just one additional output layer
to create state-of-the-art models for a wide range of tasks, such as question answering and
language inference, without substantial taskspecific architecture modifications.

\subsection{Model Architecture of BERT}
\par The model architecture of BERT is a multi-layer bidirectional Transformer encoder 
based on the Transformer architecture. Compared to the original Transformer architecture, 
BERT uses a bidirectional structure, which will be explained in more details later.

\par In addition, BERT also employs different structures for input and output 
representations. To adapt to different downstream tasks, the input representation 
should be able to handle sequences consisting of multiple sentences as single token. 
Therefore, BERT uses WordPiece embeddings for this purpose.

\par In BERT, the first token of each sequence is a special classification token ([CLS]). 
Sentences pairs are packed into a single sequence. There are two methods to differentiate 
between different sentences within a sequence. Firstly, a special separator token ([SEP]) 
is used to separate different sentences. Secondly, a learnable embedding is added to each 
token to indicate which sentence it belongs to. For a given token, its input representation 
is constructed by summing the corresponding token, segment, and position embeddings, that's 
what Fig3 shows.

\begin{figure}[htbp]
    \centerline{\includegraphics[width = 8cm]{pic/fig3.png}}
    \caption{Input Representation of BERT}
    \label{fig}
\end{figure}


\subsection{Training of BERT}

\par Training BERT is a crucial part of its development, and it involves several 
techniques, including Unsupervised Feature-based Approaches, Unsupervised 
Fine-tuning Approaches, and Transfer Learning from Supervised Data. The training 
process is divided into two parts. 

\begin{figure}[htbp]
    \centerline{\includegraphics[width = 8cm]{pic/fig4.png}}
    \caption{Training of BERT}
    \label{fig}
\end{figure}

\par The first part is Pre-Training, during which BERT 
is trained to perform two tasks.Task 1 is Masked Language Modeling (MLM), where 
15\% of the wordpiece tokens are randomly masked and replaced with a special token ([MASK]). 
BERT is then trained to predict these masked tokens.Task 2 is Next Sentence Prediction (NSP), 
where BERT is given a pair of sentences A and B and is trained to predict whether 
sentence B follows sentence A.

\par The second part is fine-tuning, where we use a small amount of 
labeled data to train BERT for specific downstream tasks. Because of the properties of 
the Transformer architecture, BERT can model downstream tasks by adjusting its inputs and 
outputs. During fine-tuning, BERT uses bidirectional self-attention mechanisms to encode 
text and to coordinate and unify its inputs and outputs for the specific task at hand.

\section{GPT}

\par GPT\cite{b3} (Generative Pre-trained Transformer) is a Transformer-based pre-trained language 
model developed by OpenAI. The development of GPT can be divided into four versions. 
GPT-1 was pre-trained on a large corpus of text and was one of the first models to 
demonstrate excellent performance in natural language processing tasks. GPT-2 is an 
improved version of GPT-1, pre-trained on larger datasets with more parameters, and 
can generate more natural and fluent text. GPT-3 is a much larger pre-trained language 
model with 175 billion parameters, achieving state-of-the-art results on various natural 
language processing tasks. As of today, GPT-4 is currently under development, expected 
to have even larger model parameters and better support for multimodal input to handle 
more complex tasks.

\subsection{GPT-1}

\par GPT-1\cite{b3} implemented a semi-supervised approach that combines unsupervised pre-training 
and supervised fine-tuning to accomplish language understanding tasks. This approach 
enables a large amount of training to form a global transformation, and only minor 
adjustments are needed to adapt to a wide range of downstream tasks.

\subsubsection{Model Architecture of GPT-1}

\par GPT-1 uses a Transformer Decoder\cite{liu2018generating} as its primary architecture, 
which has been proven to have strong performance on various tasks. However, 
depending on the specific task, we can replace or fine-tune different modules 
of the model, such as its input and output, to better adapt to the task at hand.

\begin{figure}[htbp]
    \centerline{\includegraphics[width = 8cm]{pic/fig5.png}}
    \caption{Model Architecture of GPT-1 \& The Task-specific for different tasks}
    \label{fig}
\end{figure}

\par As GPT use a Transformer Decoder, this model applies multi-headed self-attention 
operation over the input context tokens, followed by position-wise feedforward layers, 
to produce an output distribution over target tokens.

\begin{align*}
    h_0 &= UW_e + W_p \\
    h_l &= transformer\_block(h_{l-1})\ \forall i \in \left[1, n\right] \\
    P(u) &= softmax(h_n W_e^T)
\end{align*}

\par where $U = (u_{-k}, ..., u_{-1})$ is the context vector of tokens, $n$ is the number of layers, 
$W_e$ is the token embedding matrix, and $W_p$ is the position embedding matrix.

\subsubsection{Training of GPT-1}

\par The training of GPT-1 is divided into two parts.

\par The first part being unsupervised pre-training.In this process, GPT is given a 
set of unlabeled corpus tokens $\mathcal{U} = \{u_1, u_2, ..., u_n \}$ and utilizes a standard language model to maximize 
the following likelihood:

\begin{align*}
    L_1(\mathcal{U}) = \sum_{i}\log P(u_i | u_{i-k}, ..., u_{i-1};\Theta )
\end{align*}

\par where $k$ is the size of the context window, and the conditional probability $P$ is 
modeled using a neural network with parameters $\Theta$. These parameters are trained 
using stochastic gradient descent [51].

The second part is supervised fine-tuning, which involves adjusting the model parameters 
for a specific task after unsupervised pre-training. Assuming there is a labeled 
dataset $\mathcal{C} $, where each input has an associated label. The input is processed by the 
model to obtain an activation value $h^m_l$, which, when multiplied by $W_y$, can predict the 
label $y$. This allows us to maximize the following objective $L_2(\mathcal{C})$:

\begin{align*}
    P(y | x^1, x^2, ..., x^m) &= softmax(h_l^m W_y) \\
    L_2(\mathcal{C}) &= \sum_{i}\log P(y | x^1, x^2, ..., x^m)
\end{align*}

Adding a language model as an auxiliary objective during supervised fine-tuning 
can improve learning by (a) enhancing the generalization ability of the supervised 
model, and (b) accelerating convergence. Therefore, we perform a maximization operation 
on the following objective $L_3(\mathcal{C})$:

\begin{align*}
    L_3(\mathcal{C}) = L_2(\mathcal{C}) + \lambda L_1(\mathcal{C})
\end{align*}

\par where $\lambda$ is weight.

\subsection{GPT-2}

\par GPT-2\cite{b4} largely follows the architecture of its predecessor GPT-1, 
with only minor modifications. Layer normalization is moved to after the input 
of each sub-block, and additional layer normalization is added after the final 
self-attention module. A modified initialization is used to account for accumulation 
on the residual path with model depth, where the weights of residual layers are scaled 
by a factor of $1/\sqrt{N}$ at initialization, with $N$ being the number of residual layers. 
The vocabulary is expanded to 50,257, and the context size is increased from 512 to 
1024 tokens. A larger batch size of 512 is also used.

\par GPT-2 was trained on the WebText dataset, and in the original paper, 
the authors evaluated GPT-2's performance on various tasks, including Language Modeling, 
Children's book test, LAMBADA, Winograd Schema Challenge, Reading Comprehension, 
Translation, and Question Answering. As the number of parameters increased, 
GPT-2 exhibited remarkable abilities.

\subsection{GPT3}

\par The model size and amount of data used for training in GPT-3\cite{b4} have significantly 
increased. GPT-3 has 96 layers, each with 96 attention heads, for a total of 175 billion 
parameters, which is 116 times more than GPT-2. GPT-3 was trained on a 45 terabytes 
text dataset selected from Common Crawl, containing 500 billion words, which is 
10 times larger than the dataset used for GPT-2.

\par GPT-3's main selling point is few-shot learning, which means it can complete various 
downstream tasks with only a few examples as input, without fine-tuning or gradient 
updates. GPT-3 has demonstrated outstanding performance on several natural language 
processing tasks, even surpassing some specially designed models.

\subsection{GPT4}

\par GPT-4\cite{b5}'s main selling point is multimodal learning, which means it 
can accept both images and text as input, and output text. 
GPT-4 can generate descriptive or creative text based on the content 
of the images, and it can also generate relevant images based on the content 
of the text. 


%\section{Diffusion Model}
%\subsection{Foundations of Diffusion Models}
%\subsection{Applications of Diffusion Models}
%\subsection{Optimizing}

\section{Security and Robustness of LLM}
\par As the use of GPT (Generative Pre-trained Transformer) models becomes more widespread, 
people are beginning to realize the importance of addressing their robustness and security 
issues.

\par Robustness issues mainly refer to the ability of GPT models to perform in the face of noise, 
attacks, and other challenges. For example, how will the model handle input text that contains noise, 
typos, or interfering phrases? How will it perform in the face of targeted attacks, such as targeted 
adversarial sample attacks? These are all robustness issues that need to be considered.

\par Security issues mainly relate to the potential harm that GPT models 
may cause during their application. For instance, if GPT models are used to generate 
misleading statements, malicious ads, fake news, etc., they could have a negative impact 
on society. At the same time, if unauthorized individuals gain access to GPT models, 
they may use them for malicious attacks, phishing, and other illegal activities.

\par Therefore, researching the robustness and security of GPT models is essential 
to ensure their reliability and safety. This research includes but is not limited 
to adversarial sample training, robustness evaluation, model interpretability analysis, 
privacy protection, and other areas. Only by continuously researching the robustness 
and security issues of GPT models can we better address future challenges and risks, 
safeguard the reliability and security of GPT models in various fields, and promote 
their more extensive use.

\par Shortly after the launch of ChatGPT, many papers analyzing the security and 
robustness of GPT models emerged. For example, paper \cite{wang2023robustness} 
tested the performance of GPT models and previous LLM models from the 
perspectives of adversarial attacks and OOD, to see if there was significant improvement. 
Through the testing in paper, we found that GPT models showed relatively good performance 
on many security testing datasets. However, there are still issues such as vulnerability 
to attacks and inadequate guarantees of the authenticity of generated content.

\par Next, we will start with the lifecycle of an LLM (Large Language Model) and attempt 
to analyze the vulnerabilities that may exist in LLMs and the methods for discovering 
these vulnerabilities.

\subsection{Vulnerabilities}
\par At the beginning of this section, we first need to discuss the inherent issues that 
exist in LLMs. These issues cannot be fixed by the LLM itself, but can be addressed 
through expanding the training dataset and adjusting the model structure.

\subsubsection*{Performance Issues}
\par Unlike traditional software systems, the same input in an LLM does not 
always result in the same output. Performance issues in LLMs can be divided 
into two parts: factual errors and reasoning errors.

\par Factual errors refer to when the output of an LLM contradicts the truth. 
When we ask an LLM a simple common sense question, the answer we receive may 
be far from the truth. \cite{shen2023chatgpt} noted poor performance of ChatGPT in the legal 
and scientific fields due to factual errors.

\par Reasoning errors are a particularly interesting problem, 
as they indicate that LLMs are more prone to errors when performing 
logical reasoning tasks. This is because LLMs do not reason themselves, 
but generate a probabilistic answer based on the way they were trained. 
When a question is similar to what the LLM was trained on, it will have a 
higher accuracy\cite{liu2023evaluating}.

\subsubsection*{Backdoor Attack}
\par Backdoor attacks aim to inject harmful knowledge to cause errors in LLMs. 
Common backdoor attacks include poisoning the training data during the training phase 
or modifying the parameters of the model. Backdoor attacks only trigger when the 
input prompt to the model contains a corresponding trigger, which causes the LLM 
to exhibit the erroneous behavior that the attacker expects.

\par Let's first consider how to design a backdoor trigger. 
\cite{chen2021badnl} introduced three different types of backdoor triggers 
that operate at the character, word, and sentence levels. 
\cite{struppek2022rickrolling} introduces two novel backdoor triggers, one that uses 
homophones to attack and another that applies dynamic sentence attacks.

\par Next, let's consider how to implant a pre-designed trigger into a model. 
\cite{shen2021backdoor} introduced a backdoor attack on pre-trained NLP models 
that does not require any task-specific labels. \cite{kurita2020weight} introduces a method called 
RIPPLe for implementing backdoor attacks on fine-tuning datasets, while 
also implementing a technique called Embedding Surgery to enhance the survival 
rate of the backdoor.

\par In addition, there are many more complex forms of backdoor attacks. 
For example, \cite{li2021hidden} implemented backdoor attacks in more complex downstream NLP tasks, 
such as NMT and QA, by replacing the designed questions, resulting in harmful responses 
to users. \cite{struppek2022rickrolling} introduced backdoor attacks on text-based image generation tasks for 
the first time, using a teach-student method to implant the backdoor into a pre-trained 
text encoder.

\subsubsection*{Poisoning and Disinformation}

\par Poisoning attacks are quite effective in attacking deep neural networks because most 
models are fine-tuned on many public datasets that often contain many untrusted documents 
and websites, making it easy for attackers to inject harmful data. \cite{nelson2008exploiting} demonstrates 
how the poisoning attack can render a spam filter useless. Even if only 1 percent 
of the data is maliciously modified, the spam filter can easily fail. \cite{carlini2023poisoning} introduces 
two poisoning attacks, showing us how to modify a small amount of data to achieve good 
attack results, and experiments were conducted on various popular datasets.

\subsubsection*{Prompt Injection}

\par The Prompt Engineering Guide on github introduced several methods of using prompt injection to cause LLM misalignment. 
In these attacks, the attacker uses the prompt to make the LLM generate offensive and 
inappropriate content. \cite{perez2022ignore} provides two ways to attack using prompts, one called goal 
hijacking, which aims to change the potential target of the original prompt into a 
targeted one, and the other called prompt leaking, which endeavors to retrieve information 
from private prompts.

\par \cite{kang2023exploiting} explores the programmatic behavior of LLMs, demonstrating that classical 
security attack methods can also be used to attack the defense systems of LLMs. 
Instruction-based LLMs can generate convincing malicious content under malicious prompts. 
\cite{deshpande2023toxicity} proposes an attack method of assigning a persona to LLMs. For example, prompting an LLM 
to \"speak like Muhammad Ali\" can significantly increase the toxicity of its generated 
content. \cite{maus2023adversarial} develops a black-box framework for non-structured image and text generation, 
applying black-box methods to explore adversarial prompts.

\par \cite{li2023multi} claims that in previous versions of ChatGPT, personal private information 
could be directly extracted through prompts. Furthermore, they have developed a method 
that can change ChatGPT's restrictions, allowing it to answer user queries unethically. 
\cite{greshake2023more} has proposed a novel indirect prompt injection method, indicating that prompt injection 
risks may occur not only among attackers but also among users, developers, and automated 
data processing systems.

\subsubsection*{Else}
\par In fact, there are many vulnerabilities in LLMs that we have not yet 
encountered and are difficult to control. For example, there have been issues 
with the leakage of some users' information during the operation of ChatGPT, 
which highlights the importance of maintaining LLM application systems. 
In addition, we have found that LLMs may exhibit some obvious biases during operation, 
leading to answers with a certain subjective tendency. In fact, the ChatGPT model 
categorizes itself as an ENFJ personality type in the Myers-Briggs Type Indicator test. 
These potential issues may cause harm to LLMs in the future.

\subsection{Verification}

This section discusses if and how more rigorous verification can be extended to work
on LLM-based machine-learning tasks. So far, the verification or certification of LLMs
is still an emerging research area. This section will first provide a comprehensive
and systematic review of the verification techniques on various NLP models. Then
we discuss a few pioneering black-box verification methods that can be workable on
large-scale language models. These are followed by a discussion on how to extend
these efforts towards LLMs and a review of the efforts to reduce the scale of LLMs to
increase the validity of verification techniques

\subsubsection*{Verification on Natural Language Processing Models}

\par As discussed in previous sections, an attacker could generate millions of adversarial
examples by manipulating every word in a sentence. However, such methods may still
fail to address numerous unseen cases arising from exponential combinations of differ-
ent words in a text input. To overcome these limitations, another class of techniques
has emerged, grounded in the concept of "certification" or "verification"\cite{seshia2016towards,huang2017safety}.
For example, via certification or verification, these methods train the model to provide
an upper bound on the worst-case loss of perturbations, thereby offering a certificate of
robustness without necessitating the exploration of the adversarial space \cite{sinha2017certifying}. By util-
ising these certification-driven methods, we can better evaluate the modelâ€™s robustness
in the face of adversarial attacks \cite{goodfellow2017challenge}.

\subsubsection*{Black-box Verification}

\par Most existing verification techniques have specific requirements for deep 
neural networks, but with the increasing complexity and scale of LLMs, black-box 
methods have become an inevitable trend for verification\cite{wicker2018feature,wu2020game,xu2022quantifying}.
In the black-box setting, adversaries can only query the target classifier 
without knowing the underlying model or the feature representations of inputs. 
Several studies have explored more efficient methods for black-box settings, 
although most of the current approaches focus on vision models \cite{wicker2018feature,wu2020game,xu2022quantifying}.
Subsequently, an anytime algorithm was developed to approximate global robustness 
by iteratively computing lower and upper bounds \cite{ruan2019global}. Recently, some researchers 
have attempted to develop black-box verification methods for NLP models, although 
these methods are not scalable to LLMs. For example, one study introduced a framework 
for evaluating the robustness of NLP models against word substitutions \cite{la2020assessing}.


\subsubsection*{Robustness Evaluation on LLMs}

\par With the widespread adoption of LLMs, people have begun to analyze the robustness 
of these models. Cheng et al. \cite{cheng2020seq2sick} proposed a seq2seq algorithm based on a projected 
gradient method, and introduced innovative loss functions for targeted keyword attacks 
and non-overlapping adversarial examples. Weng et al. \cite{weng2022large,weng2023neural} presented a 
self-verification method that leverages the conclusion of CoT as a condition for 
constructing a new sample, and then tasks the LLM with re-predicting the original 
conditions that have been masked. This approach allows for the calculation of an 
explainable verification score based on accuracy. However, until now, there has been 
limited research on verifying LLMs.

\subsubsection*{Towards Smaller Models}

\par Due to the large number of parameters in current LLMs, 
which can reach billions, verification becomes difficult, and thus, 
we may need to use smaller LLMs. One approach is model compression, 
such as quantization \cite{frantar2022optimal,feng2023survey,nagel2020up}, 
but directly applying quantization techniques 
on LLMs can lead to performance degradation. Therefore, we still need other 
techniques to achieve this goal\cite{yao2022zeroquant, park2022nuqmm,dettmers2022gpt3}. In addition to quantization 
techniques, low-rank adaptation \cite{hu2021lora} has been shown to significantly reduce the 
number of parameters while maintaining model performance.


\section{Conclusion}
In this survey, we explored the basic structure and training principles 
of the Transformer model, with a focus on representative models such as BERT and 
GPT. We investigated the security and robustness issues of LLMs, and identified 
their major vulnerabilities. Furthermore, we examined various methods to verify LLMs.


\bibliography{mybib}
\bibliographystyle{IEEEtran}


\vspace{12pt}

\end{document}
